{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import random\n",
    "import rlcard\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "from rlcard.agents import RandomAgent\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek R1 model\n",
    "class DEEPSEEKR1:\n",
    "    def __init__(self) -> None:\n",
    "        with open(\"../config.yaml\", \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            api_key = config[\"Keys\"][\"DEEPSEEKR1PAID\"]\n",
    "\n",
    "        self.model = config[\"IDs\"][\"DEEPSEEKR1PAID\"]\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=config[\"Providers\"][\"DEEPSEEK\"],\n",
    "            api_key=api_key,\n",
    "        )\n",
    "\n",
    "    def response(self, mes):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=mes)\n",
    "\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blackjack Agent\n",
    "game_style = 'agentpro'\n",
    "now = datetime.now()\n",
    "formatted = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "storage_name = '../Agent-Pro/my_data/DeepSeek R1/' + formatted\n",
    "response = ''\n",
    "\n",
    "# Blackjack Agent\n",
    "class LlmAgent(RandomAgent):\n",
    "    \"\"\"\n",
    "    An agent that uses a Large Language Model (LLM) to make decisions in a Blackjack game.\n",
    "\n",
    "    This agent formulates a prompt based on the current game state, rules,\n",
    "    and predefined behavioral guidelines and world modeling. It then queries an LLM \n",
    "    to get a reasoned action (hit or stand). \n",
    "\n",
    "    Attributes:\n",
    "        llm (DEEPSEEKR1): An instance of the LLM used for decision making.\n",
    "        behavioral_guideline (str): A string defining the agent's playing style\n",
    "                                    or specific instructions.\n",
    "        world_modeling (str): A string containing information or assumptions\n",
    "                              about the game environment or opponent.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__(num_actions)\n",
    "        self.llm = DEEPSEEKR1()\n",
    "\n",
    "        self.behavioral_guideline = \"\"\n",
    "        self.world_modeling = \"\"\n",
    "\n",
    "\n",
    "    def extract_choice(self, text):\n",
    "        text = self.to_lower(text)\n",
    "        last_hit_index = text.rfind(\"hit\")\n",
    "        last_stand_index = text.rfind(\"stand\")\n",
    "        if last_hit_index > last_stand_index:\n",
    "            return \"hit\"\n",
    "        elif last_stand_index > last_hit_index:\n",
    "            return \"stand\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def to_lower(self, str):\n",
    "        lowercase_string = str.lower()\n",
    "        return lowercase_string\n",
    "\n",
    "    def card2string(self, cardList):\n",
    "        str = ''\n",
    "        str = ','.join(cardList)\n",
    "        str = str.replace('C', 'Club ')\n",
    "        str = str.replace('S', 'Spade ')\n",
    "        str = str.replace('H', 'Heart ')\n",
    "        str = str.replace('D', 'Diamond ')\n",
    "        str = str.replace('T', '10')\n",
    "        return str\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        Determines the agent's next action based on the current game state using an LLM.\n",
    "\n",
    "        This method constructs a detailed prompt for the LLM, including the\n",
    "        game rules (customized for a target total of 31 and dealer hitting up to 27),\n",
    "        the dealer's visible card, the agent's current hand, and any specified\n",
    "        behavioral guidelines or world modeling. The LLM's response is then parsed\n",
    "        to extract the chosen action (\"hit\" or \"stand\"). The interaction history\n",
    "        (prompt and response) is saved to a YAML file.\n",
    "\n",
    "        The game rules described in the prompt are specific:\n",
    "        - Target total: 31 (not standard Blackjack).\n",
    "        - Bust limit: Over 31.\n",
    "        - Actions: \"Stand\" or \"Hit\".\n",
    "        - Dealer hits until their total is 27 or higher.\n",
    "\n",
    "        Args:\n",
    "            state (dict): A dictionary containing the current game state.\n",
    "                          Expected to have a key 'raw_obs' which itself is a dict\n",
    "                          containing 'dealer hand' (list of dealer's cards) and\n",
    "                          'player0 hand' (list of agent's cards).\n",
    "\n",
    "        Returns:\n",
    "            int: The chosen action:\n",
    "                 - 0 for \"hit\"\n",
    "                 - 1 for \"stand\"\n",
    "                 - -1 if the LLM's response is unclear or an error occurs.\n",
    "        \"\"\"\n",
    "        \n",
    "        deal_card = state['raw_obs']['dealer hand']\n",
    "        hand_card = state['raw_obs']['player0 hand']\n",
    "        p = []\n",
    "        begin_info = \"I will describe the situation. You have to reason through this in 3-5 steps, then stop. The description begins now. You are an aggressive player of blackjack who likes to take risks to earn high returns. Please beat the dealer and win the game. \"\n",
    "\n",
    "        game_rule = \"Game Rule:\\n1. Please try to get your card total to as close to 31 as possible, without going over, and still having a higher total than the dealer.\\n2. If anyone's point total exceeds 31, he or she loses the game. \\n3. You can only choose one of the following two actions: {\\\"Stand\\\", \\\"Hit\\\"}. If you choose to Stand, you will stop taking cards and wait for the dealer to finish. If you choose to Hit, you can continue to take a card, but there is also the risk of going over 31. \\n4. After all players have completed their hands, the dealer reveals their hidden card. Dealers must hit until their cards total 27 or higher.\\n\"\n",
    "        game_info = \"The dealer's current card is {\" + self.card2string(deal_card) + \"}. The dealer has another hidden card. You don't know what it is. Your current cards are {\" + self.card2string(hand_card) + \"}. \"\n",
    "        \n",
    "        game_info += \"Behavioral guideline: \" + self.behavioral_guideline + \"\\n\"\n",
    "        game_info += \"World modeling: \" + self.world_modeling + \"\\n\"\n",
    "        game_info += \"Please read the behavioral guideline and world modeling carefully. Then you should analyze your own cards and your strategies in Self-belief and then analyze the dealer cards in World-belief. Lastly, please select your action from {\\\"Stand\\\",\\\"Hit\\\"}.### Output Format: Self-Belief is {Belief about youself}. World-Belief is {Belief about the dealer}. My action is {Your action}. Please output in the given format. Do not write anything else.\"\n",
    "\n",
    "        p.append({\"role\": \"user\", \"content\": begin_info + game_rule + game_info})\n",
    "        llm_res = self.llm.response(p)\n",
    "        p.append({\"role\": \"assistant\", \"content\": llm_res})\n",
    "\n",
    "        filename = storage_name + '.yaml'\n",
    "        with open(filename, \"a\") as yaml_file:\n",
    "            yaml.dump(p, yaml_file, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "        choice = -1\n",
    "        if self.extract_choice(llm_res) == \"hit\":\n",
    "            choice = 0\n",
    "        elif self.extract_choice(llm_res) == \"stand\":\n",
    "            choice = 1\n",
    "        else:\n",
    "            choice = -1\n",
    "            \n",
    "        return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "results = {27: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           26: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           25: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           24: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           23: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           22: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           21: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           20: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           19: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           18: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           17: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           16: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           15: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           14: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           13: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           12: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           11: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "           10: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "            9: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []},\n",
    "            8: {2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 'A': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek agent\n",
    "llm_agent = LlmAgent(num_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Modeling and Behavioral Guidelines\n",
    "# from final iteration\n",
    "\n",
    "llm_agent.behavioral_guideline = \"\"\"\n",
    "**Behavioral Guideline**  \n",
    "1. **Goal**: Maximize aggression within mathematically validated thresholds, prioritizing survival in 4-card hands while targeting 28-31 totals. Enforce unbreakable stand points to neutralize dealer compounding risks.  \n",
    "2. **Strategy**:  \n",
    "   - **Fractured Aggression Protocol**:  \n",
    "     - 3-card hands: Hit to 28+ against Q/K upcards (was 27+) to counter dealer’s 17-20 initial total threat  \n",
    "     - 4-card hands: **Mandatory stand at 29+** (no exceptions), with immediate termination of aggression upon reaching threshold  \n",
    "   - **Arithmetic Supremacy Clause**:  \n",
    "     - Triple-verify totals after every hit (upgraded from double-check) to eliminate 10% error risk  \n",
    "     - Treat 30/31 as \"fortress totals\" – instant stand regardless of dealer upcard  \n",
    "   - **Dealer Ambush Override**:  \n",
    "     - Against Q/K upcards, assume dealer’s hidden card creates initial 17+ total until proven otherwise through hand progression  \n",
    "3. **Demonstration**: Preserve *4-Card Standoff* (this game) as canonical example: 29 total with 4 cards forced stand despite dealer’s K upcard, demonstrating protocol adherence overrides outcome ambiguity.\n",
    "\"\"\"\n",
    "\n",
    "llm_agent.world_modeling = \"\"\"\n",
    "**World Modeling**1. **Rule-Description**:  \n",
    "   - **Dealer’s Compounding Horizon**: Dealers must hit until reaching 27+, creating 18-23% bust risk per additional card drawn beyond 3  \n",
    "   - **4-Card Fragility Window**: Player 4-card hands at 24-28 have 52-68% bust risk on next hit – prohibited against Q/K upcards  \n",
    "   - **Hidden Card Quantum Lock**: K upcard dealers have 33% probability of hidden card being 2-7 (initial 12-17), requiring players to build ≥28 totals  \n",
    "2. **Strategic Recognition**:  \n",
    "   - **Upcard Entanglement**:  \n",
    "     - K upcard creates dual reality: immediate dealer stand threat (initial 20) vs delayed compounding (initial 12-17)  \n",
    "     - Player must resolve uncertainty by reaching 28+ with ≤4 cards  \n",
    "   - **Threshold Elasticity 4.0**:  \n",
    "     - For every confirmed dealer initial total point below 18, reduce player stand threshold by 1.0 (not 0.75) to counter compounding acceleration  \n",
    "3. **Calibration Matrix**:  \n",
    "   - **K-Upcard Response**: 28+ (3 cards)/29+ (4 cards) with triple total verification  \n",
    "   - **Catastrophic Zones**:  \n",
    "     - 4-card 25-28 totals banned against K upcards (58-72% bust risk)  \n",
    "     - Dealer initial totals 18-20 require player minimum 29 to win \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env):\n",
    "    \"\"\"\n",
    "    Simulates a game using the provided environment, records actions taken\n",
    "    during the game, logs the final outcome, and returns the game payoffs.\n",
    "\n",
    "    The function processes the trajectory of the first player (trajectories[0])\n",
    "    to extract states and actions. \n",
    "\n",
    "    The dealer's card value is interpreted as its numerical value, or 'A' (Ace)\n",
    "    if the observed value is greater than 10.\n",
    "\n",
    "    The final game state (dealer's and player's final standing) and the outcome\n",
    "    (win, draw, lose) are logged to a YAML file, the name of which is determined\n",
    "    by the global variable 'storage_name'.\n",
    "\n",
    "    Args:\n",
    "        env: An environment object that has a 'run' method to simulate the game\n",
    "             and a 'get_payoffs' method to retrieve the results. \n",
    "\n",
    "    Returns:\n",
    "        list: A list of payoffs for the players involved in the game,\n",
    "              as returned by 'env.get_payoffs()'.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectories, payoffs = env.run(is_training=False)\n",
    "    if len(trajectories[0]) != 0:\n",
    "        final_state = []\n",
    "        action_record = []\n",
    "        state = []\n",
    "        _action_list = []\n",
    "\n",
    "        for i in range(1):\n",
    "            final_state.append(trajectories[i][-1])\n",
    "            state.append(final_state[i]['raw_obs'])\n",
    "\n",
    "        action_record.append(final_state[i]['action_record'])\n",
    "        for i in range(1, len(action_record) + 1):\n",
    "            _action_list.insert(0, action_record[-i])\n",
    "\n",
    "    last_hand_value = 0\n",
    "    for i, situation in enumerate(trajectories[0][:-1]):\n",
    "        if (i % 2 == 0): # State\n",
    "            last_hand_value = int(trajectories[0][i]['obs'][0])\n",
    "            dealer_value = int(trajectories[0][i]['obs'][1]) \n",
    "            dealer_value = dealer_value if dealer_value <= 10 else 'A'\n",
    "        else: # Action\n",
    "            action = int(situation)\n",
    "            if last_hand_value >= 8 and last_hand_value <= 27:\n",
    "                results[last_hand_value][dealer_value].append(action)\n",
    "\n",
    "    res_str = ('dealer {}, '.format(state[0]['state'][1]) +\n",
    "                'player {}, '.format(state[0]['state'][0]))\n",
    "    if payoffs[0] == 1:\n",
    "        final_res = \"win.\"\n",
    "    elif payoffs[0] == 0:\n",
    "        final_res = \"draw.\"\n",
    "    elif payoffs[0] == -1:\n",
    "        final_res = \"lose.\"\n",
    "    p = [{\"final cards\": res_str, \"final results\": final_res}]\n",
    "    filename = storage_name + '.yaml'\n",
    "    with open(filename, \"a\") as yaml_file:\n",
    "        yaml.dump(p, yaml_file, default_flow_style=False, allow_unicode=True)\n",
    "    return env.get_payoffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random games\n",
    "for i in range(1, 51):\n",
    "    now = datetime.now()\n",
    "    formatted = now.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    storage_name = '../Agent-Pro/my_data/31_run_post_training/' + formatted\n",
    "    env = rlcard.make(\n",
    "    'blackjack',\n",
    "    config={\n",
    "        'game_num_players': 1,\n",
    "        \"seed\": random.randint(0, 10**10)\n",
    "    })\n",
    "    env.set_agents([llm_agent])\n",
    "    play_game(env)\n",
    "    print(f\"Current game: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "with open(f\"../dicts/31_run_post_training/id_{id}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1635a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import rlcard\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "from rlcard.agents import RandomAgent\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "formatted = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "storage_name = '../Agent-Pro/data/training/' + formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fdea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek R1 model\n",
    "class DEEPSEEKR1:\n",
    "    def __init__(self) -> None:\n",
    "        with open(\"../config.yaml\", \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            api_key = config[\"Keys\"][\"DEEPSEEKR1PAID\"]\n",
    "\n",
    "        self.model = config[\"IDs\"][\"DEEPSEEKR1PAID\"]\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=config[\"Providers\"][\"DEEPSEEK\"],\n",
    "            api_key=api_key,\n",
    "        )\n",
    "\n",
    "    def response(self, mes):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=mes)\n",
    "\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blackjack Agent\n",
    "class LlmAgent(RandomAgent):\n",
    "    \"\"\"\n",
    "    An agent that uses a Large Language Model (LLM) to make decisions in a Blackjack game.\n",
    "\n",
    "    This agent formulates a prompt based on the current game state, rules,\n",
    "    and predefined behavioral guidelines and world modeling. It then queries an LLM \n",
    "    to get a reasoned action (hit or stand). \n",
    "\n",
    "    Attributes:\n",
    "        llm (DEEPSEEKR1): An instance of the LLM used for decision making.\n",
    "        behavioral_guideline (str): A string defining the agent's playing style\n",
    "                                    or specific instructions.\n",
    "        world_modeling (str): A string containing information or assumptions\n",
    "                              about the game environment or opponent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__(num_actions)\n",
    "        self.llm = DEEPSEEKR1()\n",
    "\n",
    "        self.behavioral_guideline = \"\"\n",
    "        self.world_modeling = \"\"\n",
    "\n",
    "    def extract_choice(self, text):\n",
    "        text = self.to_lower(text)\n",
    "        last_hit_index = text.rfind(\"hit\")\n",
    "        last_stand_index = text.rfind(\"stand\")\n",
    "        if last_hit_index > last_stand_index:\n",
    "            return \"hit\"\n",
    "        elif last_stand_index > last_hit_index:\n",
    "            return \"stand\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def to_lower(self, str):\n",
    "        lowercase_string = str.lower()\n",
    "        return lowercase_string\n",
    "\n",
    "    def card2string(self, cardList):\n",
    "        str = ''\n",
    "        str = ','.join(cardList)\n",
    "        str = str.replace('C', 'Club ')\n",
    "        str = str.replace('S', 'Spade ')\n",
    "        str = str.replace('H', 'Heart ')\n",
    "        str = str.replace('D', 'Diamond ')\n",
    "        str = str.replace('T', '10')\n",
    "        return str\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        Determines the agent's next action based on the current game state using an LLM.\n",
    "\n",
    "        This method constructs a detailed prompt for the LLM, including the\n",
    "        game rules (customized for a target total of 31 and dealer hitting up to 27),\n",
    "        the dealer's visible card, the agent's current hand, and any specified\n",
    "        behavioral guidelines or world modeling. The LLM's response is then parsed\n",
    "        to extract the chosen action (\"hit\" or \"stand\"). The interaction history\n",
    "        (prompt and response) is saved to a YAML file.\n",
    "\n",
    "        The game rules described in the prompt are specific:\n",
    "        - Target total: 31 (not standard Blackjack).\n",
    "        - Bust limit: Over 31.\n",
    "        - Actions: \"Stand\" or \"Hit\".\n",
    "        - Dealer hits until their total is 27 or higher.\n",
    "\n",
    "        Args:\n",
    "            state (dict): A dictionary containing the current game state.\n",
    "                          Expected to have a key 'raw_obs' which itself is a dict\n",
    "                          containing 'dealer hand' (list of dealer's cards) and\n",
    "                          'player0 hand' (list of agent's cards).\n",
    "\n",
    "        Returns:\n",
    "            int: The chosen action:\n",
    "                 - 0 for \"hit\"\n",
    "                 - 1 for \"stand\"\n",
    "                 - -1 if the LLM's response is unclear or an error occurs.\n",
    "        \"\"\"\n",
    "\n",
    "        deal_card = state['raw_obs']['dealer hand']\n",
    "        hand_card = state['raw_obs']['player0 hand']\n",
    "        p = []\n",
    "        begin_info = \"I will describe the situation. You have to reason through this in 3-5 steps, then stop. The description begins now. You are an aggressive player of blackjack who likes to take risks to earn high returns. Please beat the dealer and win the game. \"\n",
    "\n",
    "        game_rule = \"Game Rule:\\n1. Please try to get your card total to as close to 31 as possible, without going over, and still having a higher total than the dealer.\\n2. If anyone's point total exceeds 31, he or she loses the game. \\n3. You can only choose one of the following two actions: {\\\"Stand\\\", \\\"Hit\\\"}. If you choose to Stand, you will stop taking cards and wait for the dealer to finish. If you choose to Hit, you can continue to take a card, but there is also the risk of going over 31. \\n4. After all players have completed their hands, the dealer reveals their hidden card. Dealers must hit until their cards total 27 or higher.\\n\"\n",
    "        game_info = \"The dealer's current card is {\" + self.card2string(deal_card) + \"}. The dealer has another hidden card. You don't know what it is. Your current cards are {\" + self.card2string(hand_card) + \"}. \"\n",
    "        \n",
    "        game_info += \"Behavioral guideline: \" + self.behavioral_guideline + \"\\n\"\n",
    "        game_info += \"World modeling: \" + self.world_modeling + \"\\n\"\n",
    "        game_info += \"Please read the behavioral guideline and world modeling carefully. Then you should analyze your own cards and your strategies in Self-belief and then analyze the dealer cards in World-belief. Lastly, please select your action from {\\\"Stand\\\",\\\"Hit\\\"}.### Output Format: Self-Belief is {Belief about youself}. World-Belief is {Belief about the dealer}. My action is {Your action}. Please output in the given format. Do not write anything else.\"\n",
    "\n",
    "        p.append({\"role\": \"user\", \"content\": begin_info + game_rule + game_info})\n",
    "        llm_res = self.llm.response(p)\n",
    "        p.append({\"role\": \"assistant\", \"content\": llm_res})\n",
    "\n",
    "        filename = storage_name + '.yaml'\n",
    "        with open(filename, \"a\") as yaml_file:\n",
    "            yaml.dump(p, yaml_file, default_flow_style=False, allow_unicode=True)\n",
    "        choice = -1\n",
    "        if self.extract_choice(llm_res) == \"hit\":\n",
    "            choice = 0\n",
    "        elif self.extract_choice(llm_res) == \"stand\":\n",
    "            choice = 1\n",
    "        else:\n",
    "            choice = -1\n",
    "            \n",
    "        return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1fcc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek agent\n",
    "llm_agent = LlmAgent(num_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1f9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DEEPSEEKR1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca503b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env):\n",
    "    \"\"\"\n",
    "    Runs a single game episode using the provided environment, processes its\n",
    "    trajectory to extract final game information, and logs this information\n",
    "    to a YAML file.\n",
    "\n",
    "    The function executes a game run via 'env.run()'. It then extracts the\n",
    "    final state and action records from the first player's trajectory.\n",
    "    The dealer's and player's final states (e.g., card totals or hands)\n",
    "    are formatted into a string. The game outcome (win, draw, or lose) for\n",
    "    the first player is determined from the payoffs.\n",
    "\n",
    "    This information, specifically the formatted final card states and the\n",
    "    game result, is then appended to a YAML file. The name of this YAML file\n",
    "    is determined by the global variable 'storage_name'.\n",
    "\n",
    "    Args:\n",
    "        env: An environment object that simulates the game. It must have a\n",
    "             'run()' method.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not have an explicit return statement.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectories, payoffs = env.run(is_training=False)\n",
    "    \n",
    "    if len(trajectories[0]) != 0:\n",
    "        final_state = []\n",
    "        action_record = []\n",
    "        state = []\n",
    "        _action_list = []\n",
    "\n",
    "        for i in range(1):\n",
    "            final_state.append(trajectories[i][-1])\n",
    "            state.append(final_state[i]['raw_obs'])\n",
    "\n",
    "        action_record.append(final_state[i]['action_record'])\n",
    "        for i in range(1, len(action_record) + 1):\n",
    "            _action_list.insert(0, action_record[-i])\n",
    "\n",
    "    res_str = ('dealer {}, '.format(state[0]['state'][1]) +\n",
    "                'player {}, '.format(state[0]['state'][0]))\n",
    "    \n",
    "    if payoffs[0] == 1:\n",
    "        final_res = \"win.\"\n",
    "    elif payoffs[0] == 0:\n",
    "        final_res = \"draw.\"\n",
    "    elif payoffs[0] == -1:\n",
    "        final_res = \"lose.\"\n",
    "\n",
    "    p = [{\"final cards\": res_str, \"final results\": final_res}]\n",
    "\n",
    "    filename = storage_name + '.yaml'\n",
    "    with open(filename, \"a\") as yaml_file:\n",
    "        yaml.dump(p, yaml_file, default_flow_style=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc77235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(results, llm, old_policy):\n",
    "    \"\"\"\n",
    "    Start a learning process for an LLM-based Blackjack agent by making it\n",
    "    reflect on a past game and refine its policies.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Reads a game log from a YAML file (specified by 'results + '.yaml'').\n",
    "    2. Prompts the LLM to analyze the game record, focusing on why the game\n",
    "       was lost, problematic beliefs/actions, and underlying reasons. The game\n",
    "       rules (target 31, dealer hits to 27) are provided as context.\n",
    "    3. Prompts the LLM again to distill new \"Behavioral Guidelines\" and\n",
    "       \"World Modeling\" based on its analysis and the 'old_policy'.\n",
    "       This step aims to update the agent's strategic approach.\n",
    "    4. Logs the entire interaction (prompts and LLM responses for both analysis\n",
    "       and policy generation) into a new YAML file ('results + '_train.yaml'').\n",
    "    5. Parses the LLM's final response to separate the new behavioral guideline\n",
    "       and world modeling.\n",
    "    6. Returns these updated policy strings.\n",
    "\n",
    "    Args:\n",
    "        results (str): The base name for the YAML log files. \n",
    "        llm: An LLM interface object with a 'response(messages)' method that\n",
    "             takes a list of message dictionaries and returns a string response.\n",
    "        old_policy (tuple or list): A sequence containing two strings:\n",
    "            - 'old_policy[0]' (str): The previous behavioral guideline.\n",
    "            - 'old_policy[1]' (str): The previous world modeling.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two strings:\n",
    "            - 'behavioral_guideline' (str): The newly generated behavioral guideline.\n",
    "            - 'world_modeling' (str): The newly generated world modeling.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(results + '.yaml', 'r') as file:\n",
    "        content = file.read()\n",
    "    log = yaml.safe_load(content)\n",
    "\n",
    "    p = []\n",
    "    \n",
    "    behavioral_guideline = \"\"\n",
    "    world_modeling = \"\"\n",
    "\n",
    "    game_rule = \"Game Rule:\\n1. Please try to get your card total to as close to 31 as possible, without going over, and still having a higher total than the dealer.\\n2. If anyone's point total exceeds 31, he or she loses the game. \\n3. You can only choose one of the following two actions: {\\\"Stand\\\", \\\"Hit\\\"}. If you choose to Stand, you will stop taking cards and wait for the dealer to finish. If you choose to Hit, you can continue to take a card, but there is also the risk of going over 31. \\n4. After all players have completed their hands, the dealer reveals their hidden card. Dealers must hit until their cards total 27 or higher.\\n\"\n",
    "    setup = \"You are a seasoned blackjack expert, and you need to carefully reflect on the following record of this losing game: \"\n",
    "    game_record = f\"Game Record: {content}\\n\"\n",
    "\n",
    "    analysis_setup = \"\"\"\n",
    "    Correctness: Whether its beliefs about yourself, the game, and the dealer align with the final results.\n",
    "    Consistency: Whether each belief and action is self - contradictory.\n",
    "    Reasons: Reflect on why you lost to your dealer, which beliefs and actions are\n",
    "    problematic, and what the underlying reasons are.\n",
    "    ### Output Format: I analyze this game as follows: { Your analysis about the game and belief }.\n",
    "    \"\"\"\n",
    "    \n",
    "    p.append({\"role\": \"user\", \"content\": game_rule + setup + game_record + analysis_setup})\n",
    "    res = llm.response(p)\n",
    "    p.append({\"role\": \"assistant\", \"content\": res})\n",
    "\n",
    "    reflection = \"Policy-Level Reflection: \" + res + \"\\n\"\n",
    "    guidelines = f\"\"\"\n",
    "    Following the previous rigorous analysis, you should distill and articulate a set of\n",
    "    Behavoiral Guidelines and World Modeling. The Behavoiral Guideline is about what\n",
    "    you consider to be a more reasonable and effective behavioral strategy and\n",
    "    suggestions. World Modeling is about the description of the game and the dealer.\n",
    "\n",
    "    Here are some suggestions for you:\n",
    "\n",
    "    Behavoiral Guideline\n",
    "        1-Goal: Please summarize the detailed goal based on your reflection ...\n",
    "        2-Strategy: What kind of strategy can lead you to win in similar games ...\n",
    "        3-Demonstration: Can this game be considered a typical example to be preserved for\n",
    "        future reference ...\n",
    "    World Modeling\n",
    "        1-Rule-Description: Based on the recent reflection , describe any game rules or details\n",
    "        that are easy to overlook ...\n",
    "\n",
    "\n",
    "    Update previous policies that you used to have to better match your current understanding.\n",
    "\n",
    "    Previous Behavoiral Guideline:\n",
    "    {old_policy[0]}\n",
    "\n",
    "    Previous World Modeling:\n",
    "    {old_policy[1]}\n",
    "\n",
    "    Always start description of new Behavoiral Guideline with words Behavoiral Guideline and new World Modeling with words World Modeling.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    n = []\n",
    "    n.append({\"role\": \"user\", \"content\": game_record + reflection + guidelines})\n",
    "    res = llm.response(n)\n",
    "    n.append({\"role\": \"assistant\", \"content\": res})\n",
    "\n",
    "    filename = results + '_train' + '.yaml'\n",
    "    with open(filename, \"a\") as yaml_file:\n",
    "        yaml.dump(p, yaml_file, default_flow_style=False, allow_unicode=True)\n",
    "        yaml.dump(n, yaml_file, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "    parts = res.split(\"**World Modeling**\")\n",
    "\n",
    "    behavioral_guideline = parts[0].strip()\n",
    "    world_modeling = \"**World Modeling**\" + parts[1].strip()\n",
    "\n",
    "\n",
    "    return behavioral_guideline, world_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random games\n",
    "for i in range(13, 51):\n",
    "    now = datetime.now()\n",
    "    formatted = now.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    storage_name = '../Agent-Pro/data/training/' + formatted\n",
    "    env = rlcard.make('blackjack', config={'game_num_players': 1, \"seed\": random.randint(0, 10**10)})\n",
    "    env.set_agents([llm_agent])\n",
    "    play(env)\n",
    "\n",
    "    behavioral_guideline, world_modeling = train(storage_name, llm, [llm_agent.behavioral_guideline, llm_agent.world_modeling])\n",
    "    llm_agent.behavioral_guideline = behavioral_guideline\n",
    "    llm_agent.world_modeling = world_modeling\n",
    "\n",
    "    print(f\"Current game: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0630d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Behavioral Guideline**  \n",
      "1. **Goal**: Maximize aggression within mathematically validated thresholds, prioritizing survival in 4-card hands while targeting 28-31 totals. Enforce unbreakable stand points to neutralize dealer compounding risks.  \n",
      "2. **Strategy**:  \n",
      "   - **Fractured Aggression Protocol**:  \n",
      "     - 3-card hands: Hit to 28+ against Q/K upcards (was 27+) to counter dealer’s 17-20 initial total threat  \n",
      "     - 4-card hands: **Mandatory stand at 29+** (no exceptions), with immediate termination of aggression upon reaching threshold  \n",
      "   - **Arithmetic Supremacy Clause**:  \n",
      "     - Triple-verify totals after every hit (upgraded from double-check) to eliminate 10% error risk  \n",
      "     - Treat 30/31 as \"fortress totals\" – instant stand regardless of dealer upcard  \n",
      "   - **Dealer Ambush Override**:  \n",
      "     - Against Q/K upcards, assume dealer’s hidden card creates initial 17+ total until proven otherwise through hand progression  \n",
      "3. **Demonstration**: Preserve *4-Card Standoff* (this game) as canonical example: 29 total with 4 cards forced stand despite dealer’s K upcard, demonstrating protocol adherence overrides outcome ambiguity.\n",
      "**World Modeling**1. **Rule-Description**:  \n",
      "   - **Dealer’s Compounding Horizon**: Dealers must hit until reaching 27+, creating 18-23% bust risk per additional card drawn beyond 3  \n",
      "   - **4-Card Fragility Window**: Player 4-card hands at 24-28 have 52-68% bust risk on next hit – prohibited against Q/K upcards  \n",
      "   - **Hidden Card Quantum Lock**: K upcard dealers have 33% probability of hidden card being 2-7 (initial 12-17), requiring players to build ≥28 totals  \n",
      "2. **Strategic Recognition**:  \n",
      "   - **Upcard Entanglement**:  \n",
      "     - K upcard creates dual reality: immediate dealer stand threat (initial 20) vs delayed compounding (initial 12-17)  \n",
      "     - Player must resolve uncertainty by reaching 28+ with ≤4 cards  \n",
      "   - **Threshold Elasticity 4.0**:  \n",
      "     - For every confirmed dealer initial total point below 18, reduce player stand threshold by 1.0 (not 0.75) to counter compounding acceleration  \n",
      "3. **Calibration Matrix**:  \n",
      "   - **K-Upcard Response**: 28+ (3 cards)/29+ (4 cards) with triple total verification  \n",
      "   - **Catastrophic Zones**:  \n",
      "     - 4-card 25-28 totals banned against K upcards (58-72% bust risk)  \n",
      "     - Dealer initial totals 18-20 require player minimum 29 to win  \n",
      "\n",
      "**Key Evolutionary Changes**:  \n",
      "- Introduced *Fractured Aggression Protocol* with elevated 3-card thresholds against face cards  \n",
      "- Added *Arithmetic Supremacy Clause* to combat historical error rates  \n",
      "- Redefined dealer initial total probabilities for K upcards using quantum-lock framework  \n",
      "- Banned 4-card mid-range totals against K/Q upcards regardless of dealer hidden card assumptions\n"
     ]
    }
   ],
   "source": [
    "print(llm_agent.behavioral_guideline) \n",
    "print(llm_agent.world_modeling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
